{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbKJ0f74fhN"
      },
      "source": [
        "# Preparación de entorno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "323kk7yZ4kFn"
      },
      "source": [
        "## Instalación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_mlUorp4lo1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install -U transformers\n",
        "!pip install ydata-profiling\n",
        "!pip install squarify\n",
        "!pip install wordcloud\n",
        "!pip install nltk\n",
        "!pip install transformers torch --quiet\n",
        "!pip install h2o\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riDn-j2w4oUN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import squarify\n",
        "from PIL import Image\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from datetime import datetime, timezone\n",
        "from collections import Counter\n",
        "from math import log2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Lasso, LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, mean_squared_error, r2_score, recall_score, roc_auc_score, precision_score, make_scorer, mean_absolute_error, roc_curve, auc\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.impute import SimpleImputer\n",
        "import multiprocessing\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, StratifiedKFold, KFold\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.base import ClassifierMixin\n",
        "\n",
        "from itertools import product\n",
        "from IPython.display import HTML\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "\n",
        "from transformers import pipeline\n",
        "from typing import List, Union\n",
        "\n",
        "import h2o\n",
        "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
        "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHZjyJMQ4zmV"
      },
      "source": [
        "## Importación de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7BZZLP44456"
      },
      "outputs": [],
      "source": [
        "df= pd.read_csv('/content/technographics.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKcZK0-s49h-"
      },
      "source": [
        "## Normalización de variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVtSGhf15AY1"
      },
      "outputs": [],
      "source": [
        "# Funciones para normalizar variables\n",
        "def normalize_cols(df):\n",
        "    df = df.copy()\n",
        "    df.columns = (df.columns\n",
        "                  .str.strip()\n",
        "                  .str.lower()\n",
        "                  .str.replace(\" \", \"_\")\n",
        "                  .str.replace(\"-\", \"_\"))\n",
        "    return df\n",
        "\n",
        "def parse_dates_inplace(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
        "\n",
        "def pick_last_date_row(row, candidates):\n",
        "    for c in candidates:\n",
        "        if c in row and pd.notna(row[c]):\n",
        "            return row[c]\n",
        "    return pd.NaT\n",
        "\n",
        "def shannon_entropy_normalized(series):\n",
        "    counts = series.value_counts(dropna=False).values\n",
        "    total = counts.sum()\n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "    p = counts / total\n",
        "    ent = -(p * np.log2(p + 1e-12)).sum()\n",
        "    k = (counts > 0).sum()\n",
        "    return float(ent / np.log2(k)) if k > 1 else 0.0\n",
        "\n",
        "# Normalizar variables (aplicar funciones)\n",
        "df = normalize_cols(df)\n",
        "\n",
        "# Fechas que interesa analizar\n",
        "date_candidates = [\n",
        "    \"last_date_found\",\n",
        "    \"last_date_found_source_job_url\",\n",
        "    \"last_date_found_source_job_title\",\n",
        "    \"last_date_found_source_job_description\",\n",
        "    \"last_date_any\",\n",
        "    \"first_date_found\",\n",
        "    \"first_date_found_source_job_description\",\n",
        "    \"first_date_found_source_job_title\",\n",
        "    \"first_date_found_source_job_url\"]\n",
        "\n",
        "parse_dates_inplace(df, date_candidates)\n",
        "\n",
        "# Última fecha válida por fila\n",
        "df[\"last_date_any\"] = df.apply(lambda r: pick_last_date_row(r, date_candidates), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xswyi9f15STc"
      },
      "outputs": [],
      "source": [
        "date_cols = [\n",
        "    \"last_date_found\",\n",
        "    \"last_date_found_source_job_url\",\n",
        "    \"last_date_found_source_job_title\",\n",
        "    \"last_date_found_source_job_description\",\n",
        "    \"last_date_any\",\n",
        "    \"first_date_found\",\n",
        "    \"first_date_found_source_job_description\",\n",
        "    \"first_date_found_source_job_title\",\n",
        "    \"first_date_found_source_job_url\"]\n",
        "\n",
        "def add_days_since_and_drop_dates(df: pd.DataFrame, date_cols: list) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    now_utc = pd.Timestamp(datetime.now(timezone.utc))\n",
        "\n",
        "    for col in date_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "            new_col = f\"days_since_{col}\"\n",
        "            df[new_col] = (now_utc - df[col]).dt.days\n",
        "            max_days = df[new_col].max()\n",
        "            df[new_col].fillna(max_days + 1 if pd.notna(max_days) else 0, inplace=True)\n",
        "\n",
        "    # Eliminar columnas originales\n",
        "    df.drop(columns=date_cols, inplace=True, errors=\"ignore\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Aplicar la función\n",
        "df = add_days_since_and_drop_dates(df, date_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0qqv5nF5X6j"
      },
      "source": [
        "## Creación de variable target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahVGDab3Jqtn"
      },
      "outputs": [],
      "source": [
        "# Lista de ategorías asociadas a ZL\n",
        "zl_tech_subcategories = ['build-automation',\n",
        "                         'crm-platforms',\n",
        "                         'customer-satisfaction',\n",
        "                         'marketing-automation',\n",
        "                         'appointments-and-scheduling',\n",
        "                         'customer-data-integration',\n",
        "                         'business-intelligence-bi',\n",
        "                         'resource-scheduling',\n",
        "                         'landing-page',\n",
        "                         'data-visualization',\n",
        "                         'email-marketing-platforms'\n",
        "                         'code-free-chatbot-builders',\n",
        "                         'platform-as-a-service-paas']\n",
        "\n",
        "# Uso de zl_tech por caso\n",
        "df['zl_tech_sub'] = df['subcategory_slug'].isin(zl_tech_subcategories)\n",
        "\n",
        "# Uso de zl_tech por empresa\n",
        "df['zl_tech'] = df.groupby('company_id')['zl_tech_sub'].transform('max')\n",
        "\n",
        "# Distribución de zl_tech\n",
        "print(\"Frequency distribution of zl_tech:\")\n",
        "display(df['zl_tech'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EoPI6mJ5nte"
      },
      "source": [
        "# Análisis exploratorio de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTGEgJat5q5J",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "profile = ProfileReport(df)\n",
        "profile.to_file(output_file='Technographics.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5mtTOn355GQ"
      },
      "outputs": [],
      "source": [
        "# Tipo de tecnologías\n",
        "\n",
        "colores = [\"#FFD700\", \"#FF8C00\", \"#8B4513\"]\n",
        "colormap_naranja_marron = LinearSegmentedColormap.from_list(\"naranja_marron\", colores)\n",
        "\n",
        "palabras = (\n",
        "    df['keyword_slug']\n",
        "    .dropna()\n",
        "    .astype(str)\n",
        "    .str.replace(\"-\", \" \")\n",
        "    .str.split()\n",
        "    .sum())\n",
        "\n",
        "texto = \" \".join(palabras)\n",
        "\n",
        "# nube\n",
        "wordcloud = WordCloud(\n",
        "    stopwords=STOPWORDS,\n",
        "    background_color='white',\n",
        "    colormap=colormap_naranja_marron,\n",
        "    width=800,\n",
        "    height=400\n",
        ").generate(texto)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title (\"Technologies\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqRhGaSV5_Pe"
      },
      "outputs": [],
      "source": [
        "# Estilo\n",
        "sns.set(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Paleta uniforme\n",
        "palette = {\"True\": \"#FF8C00\", \"False\": \"#B0C4DE\"}\n",
        "\n",
        "# 1: Subcategorías\n",
        "top_subcats = df['subcategory_slug'].value_counts().head(20)\n",
        "top_df = top_subcats.reset_index()\n",
        "top_df.columns = ['subcategory_slug', 'count']\n",
        "top_df['is_zl_tech'] = top_df['subcategory_slug'].isin(zl_tech_subcategories)\n",
        "\n",
        "sns.barplot(\n",
        "    data=top_df, x=\"count\", y=\"subcategory_slug\",\n",
        "    hue=\"is_zl_tech\", dodge=False, palette={True: \"#FF8C00\", False: \"#B0C4DE\"}, ax=axes[0])\n",
        "axes[0].set_title(\"Top 20 Subcategory Slugs\")\n",
        "axes[0].set_xlabel(\"Count\")\n",
        "axes[0].set_ylabel(\"Subcategory Slug\")\n",
        "\n",
        "# Convertimos zl_tech a string para los boxplots\n",
        "df['zl_tech_str'] = df['zl_tech'].astype(str)\n",
        "\n",
        "# 2: Boxplot jobs (escala log)\n",
        "sns.boxplot(x='zl_tech_str', y='jobs', data=df, palette=palette, ax=axes[1])\n",
        "axes[1].set_title(\"Distribution of Jobs by Tech offered by ZL (Log Scale)\")\n",
        "axes[1].set_xlabel(\"Tech\")\n",
        "axes[1].set_ylabel(\"Jobs\")\n",
        "axes[1].set_yscale(\"log\")\n",
        "\n",
        "# 3: Boxplot relativa (escala normal)\n",
        "sns.boxplot(x='zl_tech_str', y='relative_occurrence_within_category_source_jobs', data=df, palette=palette, ax=axes[2])\n",
        "axes[2].set_title(\"Relative Occurrence within Category by Techs offered by ZL\")\n",
        "axes[2].set_xlabel(\"Tech\")\n",
        "axes[2].set_ylabel(\"Relative Occurrence\")\n",
        "\n",
        "# 4: Evolución de Jobs\n",
        "job_metrics = [\"jobs_last_180_days\", \"jobs_last_30_days\", \"jobs_last_7_days\"]\n",
        "df_jobs = df[['zl_tech_str'] + job_metrics]\n",
        "\n",
        "df_melted = df_jobs.melt(id_vars=\"zl_tech_str\", var_name=\"Job_Metric\", value_name=\"Count\")\n",
        "\n",
        "# Orden\n",
        "order = [\"jobs_last_180_days\", \"jobs_last_30_days\", \"jobs_last_7_days\"]\n",
        "df_melted[\"Job_Metric\"] = pd.Categorical(df_melted[\"Job_Metric\"], categories=order, ordered=True)\n",
        "\n",
        "sns.lineplot(\n",
        "    data=df_melted, x=\"Job_Metric\", y=\"Count\", hue=\"zl_tech_str\",\n",
        "    marker=\"o\", palette=palette, ax=axes[3], estimator=\"mean\")\n",
        "axes[3].set_title(\"Evolution of Job Metrics by Tech offered by ZL\")\n",
        "axes[3].set_xlabel(\"Job Metrics (180 → 30 → 7)\")\n",
        "axes[3].set_ylabel(\"Average Count\")\n",
        "axes[3].legend(title=\"Tech\")\n",
        "\n",
        "for tech in df_melted[\"zl_tech_str\"].unique():\n",
        "    subset = df_melted[df_melted[\"zl_tech_str\"] == tech].groupby(\"Job_Metric\")[\"Count\"].mean()\n",
        "    for i, (metric, val) in enumerate(subset.items()):\n",
        "        axes[3].annotate(f\"{val:.0f}\", (i, val), textcoords=\"offset points\", xytext=(0,5), ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAnXDu0a6LfF"
      },
      "source": [
        "# Búsqueda del mejor modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xE9iAiJ6Ot0"
      },
      "source": [
        "## Comparación de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1w1vZoC6RBx"
      },
      "outputs": [],
      "source": [
        "def comparar_modelos_clasificacion(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    incluir_columnas: List[str] = [],\n",
        "    test_size: float = 0.2,\n",
        "    random_state: int = 42,\n",
        "    cv: int = 5\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    # Separar X e y\n",
        "    X = df[incluir_columnas]\n",
        "    y = df[target_col]\n",
        "\n",
        "    # Codificar variable objetivo\n",
        "    if y.dtype == 'object' or y.dtype.name == 'category':\n",
        "        y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "    # Preprocesamiento para variables categóricas\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ]), X.columns)])\n",
        "\n",
        "    # Modelos con parámetros regularizados para reducir overfitting\n",
        "    modelos = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=random_state),\n",
        "        'LogisticRegression': LogisticRegression(max_iter=1000, C=0.5),\n",
        "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
        "                                 max_depth=3, subsample=0.8, colsample_bytree=0.8,\n",
        "                                 random_state=random_state),\n",
        "        'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, early_stopping=True,\n",
        "                             random_state=random_state)}\n",
        "\n",
        "    # Métricas a calcular con validación cruzada\n",
        "    scorers = {\n",
        "        'Accuracy': make_scorer(accuracy_score),\n",
        "        'Precision': make_scorer(precision_score, average='weighted'),\n",
        "        'Recall': make_scorer(recall_score, average='weighted'),\n",
        "        'F1': make_scorer(f1_score, average='weighted')}\n",
        "\n",
        "    resultados = {}\n",
        "\n",
        "    for nombre, modelo in modelos.items():\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessing', preprocessor),\n",
        "            ('classifier', modelo)])\n",
        "\n",
        "        resultados[nombre] = {}\n",
        "        for metric_name, scorer in scorers.items():\n",
        "            scores = cross_val_score(pipeline, X, y, cv=cv, scoring=scorer)\n",
        "            resultados[nombre][metric_name] = round(np.mean(scores), 4)\n",
        "\n",
        "    return pd.DataFrame(resultados).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKxKk1g-6WSf"
      },
      "outputs": [],
      "source": [
        "# A nivel académico: con todas las variables disponibles\n",
        "\n",
        "comparar_modelos_clasificacion(df,\n",
        "                               target_col='zl_tech',\n",
        "                               incluir_columnas=['days_since_first_date_found', 'days_since_last_date_found_source_job_url','days_since_last_date_found_source_job_title',\n",
        "                                                 'days_since_last_date_found_source_job_description','days_since_last_date_any','days_since_first_date_found_source_job_description',\n",
        "                                                 'days_since_first_date_found_source_job_title','days_since_first_date_found_source_job_url', 'keyword_id',\n",
        "                                                 'confidence', 'is_recruiting_agency', 'jobs','jobs_last_180_days', 'jobs_last_30_days','jobs_last_7_days','jobs_source_description',\n",
        "                                                 'jobs_source_description_last_180_days','jobs_source_description_last_30_days','jobs_source_description_last_7_days', 'jobs_source_title',\n",
        "                                                 'jobs_source_title_last_180_days', 'jobs_source_title_last_30_days', 'jobs_source_title_last_7_days','jobs_source_url',\n",
        "                                                 'jobs_source_url_last_180_days', 'jobs_source_url_last_30_days','jobs_source_url_last_7_days', 'technology_rank_source_jobs',\n",
        "                                                 'technology_rank_180_days_source_jobs','rank_last_date_found_source_job_url', 'rank_1_tie_source_jobs', 'rank_180_days_tie_source_jobs',\n",
        "                                                 'relative_occurrence_within_category_source_jobs','relative_occurrence_within_category_180_days_source_jobs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgRI6EMy6dx4"
      },
      "outputs": [],
      "source": [
        "# A nivel práctico: variables que puedo obtener de mis leads\n",
        "comparar_modelos_clasificacion(df,\n",
        "                               target_col='zl_tech',\n",
        "                               incluir_columnas=['is_recruiting_agency',\n",
        "                                                 'jobs_last_180_days', 'jobs_last_30_days', 'jobs_last_7_days',\n",
        "                                                 'jobs_source_description_last_180_days',\n",
        "                                                 'jobs_source_description_last_30_days',\n",
        "                                                 'jobs_source_description_last_7_days',\n",
        "                                                 'jobs_source_title_last_180_days',\n",
        "                                                 'jobs_source_title_last_30_days', 'jobs_source_title_last_7_days',\n",
        "                                                 'jobs_source_url_last_180_days',\n",
        "                                                 'jobs_source_url_last_30_days',\n",
        "                                                 'jobs_source_url_last_7_days'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOhZjdst6jx0"
      },
      "source": [
        "## Efecto de cada variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE-dsBe_6kZo"
      },
      "outputs": [],
      "source": [
        "# A nivel académico: usaré Random forest - Da el mejor resultado preliminar\n",
        "\n",
        "target='zl_tech'\n",
        "incluir_a = ['days_since_first_date_found', 'days_since_last_date_found_source_job_url',\n",
        "             'days_since_last_date_found_source_job_title', 'days_since_last_date_found_source_job_description',\n",
        "             'days_since_last_date_any','days_since_first_date_found_source_job_description',\n",
        "             'days_since_first_date_found_source_job_title','days_since_first_date_found_source_job_url', 'keyword_id',\n",
        "             'confidence', 'is_recruiting_agency', 'jobs','jobs_last_180_days', 'jobs_last_30_days',\n",
        "             'jobs_last_7_days','jobs_source_description', 'jobs_source_description_last_180_days',\n",
        "             'jobs_source_description_last_30_days','jobs_source_description_last_7_days', 'jobs_source_title',\n",
        "             'jobs_source_title_last_180_days', 'jobs_source_title_last_30_days', 'jobs_source_title_last_7_days',\n",
        "             'jobs_source_url', 'jobs_source_url_last_180_days', 'jobs_source_url_last_30_days',\n",
        "             'jobs_source_url_last_7_days', 'technology_rank_source_jobs','technology_rank_180_days_source_jobs',\n",
        "             'rank_last_date_found_source_job_url', 'rank_1_tie_source_jobs', 'rank_180_days_tie_source_jobs',\n",
        "             'relative_occurrence_within_category_source_jobs',\n",
        "             'relative_occurrence_within_category_180_days_source_jobs']\n",
        "\n",
        "X_a = df[incluir_a]\n",
        "y_a = df[target].astype(int)\n",
        "\n",
        "# One-hot encoding\n",
        "X_encoded_a = pd.get_dummies(X_a, drop_first=True)\n",
        "\n",
        "# Train/Test\n",
        "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(\n",
        "    X_encoded_a, y_a, test_size=0.2, random_state=42, stratify=y_a)\n",
        "\n",
        "# Modelo RandomForest\n",
        "rf_a = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    random_state=42)\n",
        "rf_a.fit(X_train_a, y_train_a)\n",
        "\n",
        "# Predicciones con probabilidad\n",
        "y_pred_proba_a = rf_a.predict_proba(X_test_a)[:,1]\n",
        "\n",
        "# Ajuste de threshold\n",
        "threshold = 0.3\n",
        "y_pred_adj_a = (y_pred_proba_a >= threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "print(\"\\nMétricas con threshold ajustado:\")\n",
        "print(f\"Accuracy : {accuracy_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"F1-score : {f1_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "\n",
        "# Curva ROC para analizar tradeoff\n",
        "fpr, tpr, thresholds = roc_curve(y_test_a, y_pred_proba_a)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=\"RandomForest\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Curva ROC - RandomForest\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Umbral óptimo según F1\n",
        "f1_scores = []\n",
        "for t in thresholds:\n",
        "    y_temp_a = (y_pred_proba_a >= t).astype(int)\n",
        "    f1_scores.append(f1_score(y_test_a, y_temp_a))\n",
        "opt_threshold_a = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"\\n Threshold óptimo (según F1): {opt_threshold_a:.3f}\")\n",
        "\n",
        "# Importancia de variables\n",
        "importances_a = rf_a.feature_importances_\n",
        "features_a = X_encoded_a.columns\n",
        "\n",
        "feat_importances_a = pd.DataFrame({\n",
        "    \"Variable\": features_a,\n",
        "    \"Importancia\": importances_a}).sort_values(by=\"Importancia\", ascending=False)\n",
        "\n",
        "print(\"\\n Variables más influyentes en el modelo:\")\n",
        "print(feat_importances_a.head(30))\n",
        "\n",
        "# Gráfico\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feat_importances_a[\"Variable\"].head(30), feat_importances_a[\"Importancia\"].head(30))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Importancia\")\n",
        "plt.title(\"Top 15 Variables más influyentes - RandomForest\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4H8Au897fwY"
      },
      "outputs": [],
      "source": [
        "# A nivel práctico: usaré Random Forest\n",
        "\n",
        "incluir_p = ['is_recruiting_agency',\n",
        "             'jobs_last_180_days', 'jobs_last_30_days', 'jobs_last_7_days',\n",
        "             'jobs_source_description_last_180_days',\n",
        "             'jobs_source_description_last_30_days',\n",
        "             'jobs_source_description_last_7_days',\n",
        "             'jobs_source_title_last_180_days',\n",
        "             'jobs_source_title_last_30_days', 'jobs_source_title_last_7_days',\n",
        "             'jobs_source_url_last_180_days',\n",
        "             'jobs_source_url_last_30_days',\n",
        "             'jobs_source_url_last_7_days']\n",
        "\n",
        "X_p = df[incluir_p]\n",
        "y_p = df[target].astype(int)\n",
        "\n",
        "# One-hot encoding\n",
        "X_encoded_p = pd.get_dummies(X_p, drop_first=True)\n",
        "\n",
        "# Train/Test\n",
        "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
        "    X_encoded_p, y_p, test_size=0.2, random_state=42, stratify=y_p)\n",
        "\n",
        "# Modelo RandomForest\n",
        "rf_p = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    random_state=42)\n",
        "rf_p.fit(X_train_p, y_train_p)\n",
        "\n",
        "# Predicciones con probabilidad\n",
        "y_pred_proba_p = rf_p.predict_proba(X_test_p)[:,1]\n",
        "\n",
        "# Ajuste de threshold\n",
        "threshold = 0.3\n",
        "y_pred_adj_p = (y_pred_proba_p >= threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "print(\"\\nMétricas con threshold ajustado:\")\n",
        "print(f\"Accuracy : {accuracy_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"F1-score : {f1_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "\n",
        "# Curva ROC para analizar tradeoff\n",
        "fpr, tpr, thresholds = roc_curve(y_test_p, y_pred_proba_p)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=\"RandomForest\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Curva ROC - RandomForest\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Umbral óptimo según F1\n",
        "f1_scores = []\n",
        "for t in thresholds:\n",
        "    y_temp_p = (y_pred_proba_p >= t).astype(int)\n",
        "    f1_scores.append(f1_score(y_test_p, y_temp_p))\n",
        "opt_threshold_p = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"\\n Threshold óptimo (según F1): {opt_threshold_p:.3f}\")\n",
        "\n",
        "# Importancia de variables\n",
        "importances_p = rf_p.feature_importances_\n",
        "features_p = X_encoded_p.columns\n",
        "\n",
        "feat_importances_p = pd.DataFrame({\n",
        "    \"Variable\": features_p,\n",
        "    \"Importancia\": importances_p}).sort_values(by=\"Importancia\", ascending=False)\n",
        "\n",
        "print(\"\\n Variables más influyentes en el modelo:\")\n",
        "print(feat_importances_p.head(30))\n",
        "\n",
        "# Gráfico\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feat_importances_p[\"Variable\"].head(30), feat_importances_p[\"Importancia\"].head(30))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Importancia\")\n",
        "plt.title(\"Top 15 Variables más influyentes - RandomForest\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbQtmYiv7jsv"
      },
      "source": [
        "## Optimizar modelo seleccionando variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i-UpU0u7mYl"
      },
      "outputs": [],
      "source": [
        "#Modelo Académico\n",
        "incluir_a = ['days_since_first_date_found',\n",
        "             'days_since_first_date_found_source_job_description',\n",
        "             'keyword_id',\n",
        "             'days_since_last_date_any',\n",
        "             'days_since_last_date_found_source_job_description',\n",
        "             'jobs_source_description',\n",
        "             'relative_occurrence_within_category_source_jobs',\n",
        "             'jobs',\n",
        "             'jobs_last_180_days']\n",
        "\n",
        "X_a = df[incluir_a]\n",
        "y_a = df[target].astype(int)\n",
        "\n",
        "# One-hot encoding\n",
        "X_encoded_a = pd.get_dummies(X_a, drop_first=True)\n",
        "\n",
        "# Train/Test\n",
        "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(\n",
        "    X_encoded_a, y_a, test_size=0.2, random_state=42, stratify=y_a)\n",
        "\n",
        "# Modelo RandomForest\n",
        "rf_a = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    random_state=42)\n",
        "rf_a.fit(X_train_a, y_train_a)\n",
        "\n",
        "# Predicciones con probabilidad\n",
        "y_pred_proba_a = rf_a.predict_proba(X_test_a)[:,1]\n",
        "\n",
        "# Ajuste de threshold\n",
        "threshold = 0.3\n",
        "y_pred_adj_a = (y_pred_proba_a >= threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "print(\"\\nMétricas con threshold ajustado:\")\n",
        "print(f\"Accuracy : {accuracy_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"F1-score : {f1_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "\n",
        "# Curva ROC para analizar tradeoff\n",
        "fpr, tpr, thresholds = roc_curve(y_test_a, y_pred_proba_a)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=\"RandomForest\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Curva ROC - RandomForest\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Umbral óptimo según F1\n",
        "f1_scores = []\n",
        "for t in thresholds:\n",
        "    y_temp_a = (y_pred_proba_a >= t).astype(int)\n",
        "    f1_scores.append(f1_score(y_test_a, y_temp_a))\n",
        "opt_threshold_a = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"\\n Threshold óptimo (según F1): {opt_threshold_a:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQmoYgXd7rmO"
      },
      "outputs": [],
      "source": [
        "#Modelo Práctico optimizado\n",
        "incluir_p = ['jobs_source_description_last_180_days',\n",
        "             'jobs_last_180_days',\n",
        "             'jobs_last_30_days',\n",
        "             'jobs_source_description_last_30_days',\n",
        "             'jobs_source_description_last_7_days',\n",
        "             'jobs_last_7_days',\n",
        "             'jobs_source_title_last_180_days']\n",
        "\n",
        "X_p = df[incluir_p]\n",
        "y_p = df[target].astype(int)\n",
        "\n",
        "# One-hot encoding\n",
        "X_encoded_p = pd.get_dummies(X_p, drop_first=True)\n",
        "\n",
        "# Train/Test\n",
        "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
        "    X_encoded_p, y_p, test_size=0.2, random_state=42, stratify=y_p)\n",
        "\n",
        "#Preprocesamiento\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', StandardScaler(), X_p.select_dtypes(include=np.number).columns),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), X_p.select_dtypes(include='object').columns)])\n",
        "\n",
        "# Crear el pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', rf_p)])\n",
        "\n",
        "# Entrenar el pipeline con todos los datos\n",
        "pipeline.fit(X_p, y_p)\n",
        "\n",
        "# Modelo RandomForest\n",
        "rf_p = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    random_state=42)\n",
        "rf_p.fit(X_train_p, y_train_p)\n",
        "\n",
        "# Predicciones con probabilidad\n",
        "y_pred_proba_p = rf_p.predict_proba(X_test_p)[:,1]\n",
        "\n",
        "# Ajuste de threshold\n",
        "threshold = 0.3\n",
        "y_pred_adj_p = (y_pred_proba_p >= threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "print(\"\\nMétricas con threshold ajustado:\")\n",
        "print(f\"Accuracy : {accuracy_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"F1-score : {f1_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "\n",
        "# Curva ROC para analizar tradeoff\n",
        "fpr, tpr, thresholds = roc_curve(y_test_p, y_pred_proba_p)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=\"RandomForest\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Curva ROC - RandomForest\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Umbral óptimo según F1\n",
        "f1_scores = []\n",
        "for t in thresholds:\n",
        "    y_temp_p = (y_pred_proba_p >= t).astype(int)\n",
        "    f1_scores.append(f1_score(y_test_p, y_temp_p))\n",
        "opt_threshold_p = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"\\n Threshold óptimo (según F1): {opt_threshold_p:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5C6Q6ZX72r0"
      },
      "source": [
        "# Productivización del modelo (práctico)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ETju3x-77G6"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken 325KD5cOKlBGzVkYLxGGy3gjqIn_7KCV6t5vbmU7dKmC5oH56\n",
        "app = Flask(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU0_3FoD77vR"
      },
      "outputs": [],
      "source": [
        "# Guardar el modelo entrenado\n",
        "joblib.dump(pipeline, 'modelo_practico_optimizado.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB9JeWp37-Fi"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo entrenado\n",
        "\n",
        "try:\n",
        "    pipeline = joblib.load('modelo_practico_optimizado.pkl')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: modelo_practico_optimizado.pkl not found.\")\n",
        "    pipeline = None\n",
        "\n",
        "# Ruta\n",
        "if pipeline is not None:\n",
        "    @app.route('/predigo', methods=['POST'])\n",
        "    def predigo():\n",
        "        data = request.get_json(force=True)\n",
        "        df_predict = pd.DataFrame([data])\n",
        "        preds = pipeline.predict(df_predict)\n",
        "        return jsonify({'prediction': int(preds[0])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8unUyam8EBO"
      },
      "outputs": [],
      "source": [
        "# Abrir un túnel público\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"La API está disponible en:\", public_url)\n",
        "\n",
        "# Levantar Flask\n",
        "app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VDbKJ0f74fhN",
        "323kk7yZ4kFn",
        "VKcZK0-s49h-",
        "E0qqv5nF5X6j",
        "6EoPI6mJ5nte",
        "eAnXDu0a6LfF",
        "8xE9iAiJ6Ot0",
        "WOhZjdst6jx0",
        "hbQtmYiv7jsv",
        "j5C6Q6ZX72r0"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}