{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbKJ0f74fhN"
      },
      "source": [
        "# Preparación de entorno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "323kk7yZ4kFn"
      },
      "source": [
        "## Instalación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "s_mlUorp4lo1",
        "outputId": "c85ad13f-a89b-4659-f137-6d9fea3db135",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-43871566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -U transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install ydata-profiling'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install squarify'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install wordcloud'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    501\u001b[0m             make_files(\n\u001b[1;32m    502\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mlocate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;34m\"\"\"Return a path-like object for this path\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mlocate_file\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mread_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mlocate_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install -U transformers\n",
        "!pip install ydata-profiling\n",
        "!pip install squarify\n",
        "!pip install wordcloud\n",
        "!pip install nltk\n",
        "!pip install transformers torch --quiet\n",
        "!pip install h2o\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riDn-j2w4oUN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import squarify\n",
        "from PIL import Image\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from datetime import datetime, timezone\n",
        "from collections import Counter\n",
        "from math import log2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Lasso, LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, mean_squared_error, r2_score, recall_score, roc_auc_score, precision_score, make_scorer, mean_absolute_error, roc_curve, auc\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.impute import SimpleImputer\n",
        "import multiprocessing\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, StratifiedKFold, KFold\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.base import ClassifierMixin\n",
        "\n",
        "from itertools import product\n",
        "from IPython.display import HTML\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "\n",
        "from transformers import pipeline\n",
        "from typing import List, Union\n",
        "\n",
        "import h2o\n",
        "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
        "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHZjyJMQ4zmV"
      },
      "source": [
        "## Importación de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7BZZLP44456"
      },
      "outputs": [],
      "source": [
        "df= pd.read_csv('/content/technographics.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKcZK0-s49h-"
      },
      "source": [
        "## Normalización de variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVtSGhf15AY1"
      },
      "outputs": [],
      "source": [
        "# Funciones para normalizar variables\n",
        "def normalize_cols(df):\n",
        "    df = df.copy()\n",
        "    df.columns = (df.columns\n",
        "                  .str.strip()\n",
        "                  .str.lower()\n",
        "                  .str.replace(\" \", \"_\")\n",
        "                  .str.replace(\"-\", \"_\"))\n",
        "    return df\n",
        "\n",
        "def parse_dates_inplace(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
        "\n",
        "def pick_last_date_row(row, candidates):\n",
        "    for c in candidates:\n",
        "        if c in row and pd.notna(row[c]):\n",
        "            return row[c]\n",
        "    return pd.NaT\n",
        "\n",
        "def shannon_entropy_normalized(series):\n",
        "    counts = series.value_counts(dropna=False).values\n",
        "    total = counts.sum()\n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "    p = counts / total\n",
        "    ent = -(p * np.log2(p + 1e-12)).sum()\n",
        "    k = (counts > 0).sum()\n",
        "    return float(ent / np.log2(k)) if k > 1 else 0.0\n",
        "\n",
        "# Normalizar variables (aplicar funciones)\n",
        "df = normalize_cols(df)\n",
        "\n",
        "# Fechas que interesa analizar\n",
        "date_candidates = [\n",
        "    \"last_date_found\",\n",
        "    \"last_date_found_source_job_url\",\n",
        "    \"last_date_found_source_job_title\",\n",
        "    \"last_date_found_source_job_description\",\n",
        "    \"last_date_any\",\n",
        "    \"first_date_found\",\n",
        "    \"first_date_found_source_job_description\",\n",
        "    \"first_date_found_source_job_title\",\n",
        "    \"first_date_found_source_job_url\"]\n",
        "\n",
        "parse_dates_inplace(df, date_candidates)\n",
        "\n",
        "# Última fecha válida por fila\n",
        "df[\"last_date_any\"] = df.apply(lambda r: pick_last_date_row(r, date_candidates), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xswyi9f15STc"
      },
      "outputs": [],
      "source": [
        "date_cols = [\n",
        "    \"last_date_found\",\n",
        "    \"last_date_found_source_job_url\",\n",
        "    \"last_date_found_source_job_title\",\n",
        "    \"last_date_found_source_job_description\",\n",
        "    \"last_date_any\",\n",
        "    \"first_date_found\",\n",
        "    \"first_date_found_source_job_description\",\n",
        "    \"first_date_found_source_job_title\",\n",
        "    \"first_date_found_source_job_url\"]\n",
        "\n",
        "def add_days_since_and_drop_dates(df: pd.DataFrame, date_cols: list) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    now_utc = pd.Timestamp(datetime.now(timezone.utc))\n",
        "\n",
        "    for col in date_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "            new_col = f\"days_since_{col}\"\n",
        "            df[new_col] = (now_utc - df[col]).dt.days\n",
        "            max_days = df[new_col].max()\n",
        "            df[new_col].fillna(max_days + 1 if pd.notna(max_days) else 0, inplace=True)\n",
        "\n",
        "    # Eliminar columnas originales\n",
        "    df.drop(columns=date_cols, inplace=True, errors=\"ignore\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Aplicar la función\n",
        "df = add_days_since_and_drop_dates(df, date_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0qqv5nF5X6j"
      },
      "source": [
        "## Creación de variable target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahVGDab3Jqtn"
      },
      "outputs": [],
      "source": [
        "# Lista de ategorías asociadas a ZL\n",
        "zl_tech_subcategories = ['build-automation',\n",
        "                         'crm-platforms',\n",
        "                         'customer-satisfaction',\n",
        "                         'marketing-automation',\n",
        "                         'appointments-and-scheduling',\n",
        "                         'customer-data-integration',\n",
        "                         'business-intelligence-bi',\n",
        "                         'resource-scheduling',\n",
        "                         'landing-page',\n",
        "                         'data-visualization',\n",
        "                         'email-marketing-platforms'\n",
        "                         'code-free-chatbot-builders',\n",
        "                         'platform-as-a-service-paas']\n",
        "\n",
        "# Uso de zl_tech por caso\n",
        "df['zl_tech_sub'] = df['subcategory_slug'].isin(zl_tech_subcategories)\n",
        "\n",
        "# Uso de zl_tech por empresa\n",
        "df['zl_tech'] = df.groupby('company_id')['zl_tech_sub'].transform('max')\n",
        "\n",
        "# Distribución de zl_tech\n",
        "print(\"Frequency distribution of zl_tech:\")\n",
        "display(df['zl_tech'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EoPI6mJ5nte"
      },
      "source": [
        "# Análisis exploratorio de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTGEgJat5q5J",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "profile = ProfileReport(df)\n",
        "profile.to_file(output_file='Technographics.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5mtTOn355GQ"
      },
      "outputs": [],
      "source": [
        "# Tipo de tecnologías\n",
        "\n",
        "colores = [\"#FFD700\", \"#FF8C00\", \"#8B4513\"]\n",
        "colormap_naranja_marron = LinearSegmentedColormap.from_list(\"naranja_marron\", colores)\n",
        "\n",
        "palabras = (\n",
        "    df['keyword_slug']\n",
        "    .dropna()\n",
        "    .astype(str)\n",
        "    .str.replace(\"-\", \" \")\n",
        "    .str.split()\n",
        "    .sum())\n",
        "\n",
        "texto = \" \".join(palabras)\n",
        "\n",
        "# nube\n",
        "wordcloud = WordCloud(\n",
        "    stopwords=STOPWORDS,\n",
        "    background_color='white',\n",
        "    colormap=colormap_naranja_marron,\n",
        "    width=800,\n",
        "    height=400\n",
        ").generate(texto)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title (\"Technologies\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqRhGaSV5_Pe"
      },
      "outputs": [],
      "source": [
        "# Estilo\n",
        "sns.set(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Paleta uniforme\n",
        "palette = {\"True\": \"#FF8C00\", \"False\": \"#B0C4DE\"}\n",
        "\n",
        "# 1: Subcategorías\n",
        "top_subcats = df['subcategory_slug'].value_counts().head(20)\n",
        "top_df = top_subcats.reset_index()\n",
        "top_df.columns = ['subcategory_slug', 'count']\n",
        "top_df['is_zl_tech'] = top_df['subcategory_slug'].isin(zl_tech_subcategories)\n",
        "\n",
        "sns.barplot(\n",
        "    data=top_df, x=\"count\", y=\"subcategory_slug\",\n",
        "    hue=\"is_zl_tech\", dodge=False, palette={True: \"#FF8C00\", False: \"#B0C4DE\"}, ax=axes[0])\n",
        "axes[0].set_title(\"Top 20 Subcategory Slugs\")\n",
        "axes[0].set_xlabel(\"Count\")\n",
        "axes[0].set_ylabel(\"Subcategory Slug\")\n",
        "\n",
        "# Convertimos zl_tech a string para los boxplots\n",
        "df['zl_tech_str'] = df['zl_tech'].astype(str)\n",
        "\n",
        "# 2: Boxplot jobs (escala log)\n",
        "sns.boxplot(x='zl_tech_str', y='jobs', data=df, palette=palette, ax=axes[1])\n",
        "axes[1].set_title(\"Distribution of Jobs by Tech offered by ZL (Log Scale)\")\n",
        "axes[1].set_xlabel(\"Tech\")\n",
        "axes[1].set_ylabel(\"Jobs\")\n",
        "axes[1].set_yscale(\"log\")\n",
        "\n",
        "# 3: Boxplot relativa (escala normal)\n",
        "sns.boxplot(x='zl_tech_str', y='relative_occurrence_within_category_source_jobs', data=df, palette=palette, ax=axes[2])\n",
        "axes[2].set_title(\"Relative Occurrence within Category by Techs offered by ZL\")\n",
        "axes[2].set_xlabel(\"Tech\")\n",
        "axes[2].set_ylabel(\"Relative Occurrence\")\n",
        "\n",
        "# 4: Evolución de Jobs\n",
        "job_metrics = [\"jobs_last_180_days\", \"jobs_last_30_days\", \"jobs_last_7_days\"]\n",
        "df_jobs = df[['zl_tech_str'] + job_metrics]\n",
        "\n",
        "df_melted = df_jobs.melt(id_vars=\"zl_tech_str\", var_name=\"Job_Metric\", value_name=\"Count\")\n",
        "\n",
        "# Orden\n",
        "order = [\"jobs_last_180_days\", \"jobs_last_30_days\", \"jobs_last_7_days\"]\n",
        "df_melted[\"Job_Metric\"] = pd.Categorical(df_melted[\"Job_Metric\"], categories=order, ordered=True)\n",
        "\n",
        "sns.lineplot(\n",
        "    data=df_melted, x=\"Job_Metric\", y=\"Count\", hue=\"zl_tech_str\",\n",
        "    marker=\"o\", palette=palette, ax=axes[3], estimator=\"mean\")\n",
        "axes[3].set_title(\"Evolution of Job Metrics by Tech offered by ZL\")\n",
        "axes[3].set_xlabel(\"Job Metrics (180 → 30 → 7)\")\n",
        "axes[3].set_ylabel(\"Average Count\")\n",
        "axes[3].legend(title=\"Tech\")\n",
        "\n",
        "for tech in df_melted[\"zl_tech_str\"].unique():\n",
        "    subset = df_melted[df_melted[\"zl_tech_str\"] == tech].groupby(\"Job_Metric\")[\"Count\"].mean()\n",
        "    for i, (metric, val) in enumerate(subset.items()):\n",
        "        axes[3].annotate(f\"{val:.0f}\", (i, val), textcoords=\"offset points\", xytext=(0,5), ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAnXDu0a6LfF"
      },
      "source": [
        "# Búsqueda del mejor modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xE9iAiJ6Ot0"
      },
      "source": [
        "## Comparación de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1w1vZoC6RBx"
      },
      "outputs": [],
      "source": [
        "def comparar_modelos_clasificacion(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    incluir_columnas: List[str] = [],\n",
        "    test_size: float = 0.2,\n",
        "    random_state: int = 42,\n",
        "    cv: int = 5\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    # Separar X e y\n",
        "    X = df[incluir_columnas]\n",
        "    y = df[target_col]\n",
        "\n",
        "    # Codificar variable objetivo\n",
        "    if y.dtype == 'object' or y.dtype.name == 'category':\n",
        "        y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "    # Preprocesamiento para variables categóricas\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ]), X.columns)])\n",
        "\n",
        "    # Modelos con parámetros regularizados para reducir overfitting\n",
        "    modelos = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=random_state),\n",
        "        'LogisticRegression': LogisticRegression(max_iter=1000, C=0.5),\n",
        "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
        "                                 max_depth=3, subsample=0.8, colsample_bytree=0.8,\n",
        "                                 random_state=random_state),\n",
        "        'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, early_stopping=True,\n",
        "                             random_state=random_state)}\n",
        "\n",
        "    # Métricas a calcular con validación cruzada\n",
        "    scorers = {\n",
        "        'Accuracy': make_scorer(accuracy_score),\n",
        "        'Precision': make_scorer(precision_score, average='weighted'),\n",
        "        'Recall': make_scorer(recall_score, average='weighted'),\n",
        "        'F1': make_scorer(f1_score, average='weighted')}\n",
        "\n",
        "    resultados = {}\n",
        "\n",
        "    for nombre, modelo in modelos.items():\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessing', preprocessor),\n",
        "            ('classifier', modelo)])\n",
        "\n",
        "        resultados[nombre] = {}\n",
        "        for metric_name, scorer in scorers.items():\n",
        "            scores = cross_val_score(pipeline, X, y, cv=cv, scoring=scorer)\n",
        "            resultados[nombre][metric_name] = round(np.mean(scores), 4)\n",
        "\n",
        "    return pd.DataFrame(resultados).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKxKk1g-6WSf"
      },
      "outputs": [],
      "source": [
        "# A nivel académico: con todas las variables disponibles\n",
        "\n",
        "comparar_modelos_clasificacion(df,\n",
        "                               target_col='zl_tech',\n",
        "                               incluir_columnas=['days_since_first_date_found', 'days_since_last_date_found_source_job_url','days_since_last_date_found_source_job_title',\n",
        "                                                 'days_since_last_date_found_source_job_description','days_since_last_date_any','days_since_first_date_found_source_job_description',\n",
        "                                                 'days_since_first_date_found_source_job_title','days_since_first_date_found_source_job_url', 'keyword_id',\n",
        "                                                 'confidence', 'is_recruiting_agency', 'jobs','jobs_last_180_days', 'jobs_last_30_days','jobs_last_7_days','jobs_source_description',\n",
        "                                                 'jobs_source_description_last_180_days','jobs_source_description_last_30_days','jobs_source_description_last_7_days', 'jobs_source_title',\n",
        "                                                 'jobs_source_title_last_180_days', 'jobs_source_title_last_30_days', 'jobs_source_title_last_7_days','jobs_source_url',\n",
        "                                                 'jobs_source_url_last_180_days', 'jobs_source_url_last_30_days','jobs_source_url_last_7_days', 'technology_rank_source_jobs',\n",
        "                                                 'technology_rank_180_days_source_jobs','rank_last_date_found_source_job_url', 'rank_1_tie_source_jobs', 'rank_180_days_tie_source_jobs',\n",
        "                                                 'relative_occurrence_within_category_source_jobs','relative_occurrence_within_category_180_days_source_jobs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgRI6EMy6dx4"
      },
      "outputs": [],
      "source": [
        "# A nivel práctico: variables que puedo obtener de mis leads\n",
        "comparar_modelos_clasificacion(df,\n",
        "                               target_col='zl_tech',\n",
        "                               incluir_columnas=['is_recruiting_agency',\n",
        "                                                 'jobs_last_180_days', 'jobs_last_30_days', 'jobs_last_7_days',\n",
        "                                                 'jobs_source_description_last_180_days',\n",
        "                                                 'jobs_source_description_last_30_days',\n",
        "                                                 'jobs_source_description_last_7_days',\n",
        "                                                 'jobs_source_title_last_180_days',\n",
        "                                                 'jobs_source_title_last_30_days', 'jobs_source_title_last_7_days',\n",
        "                                                 'jobs_source_url_last_180_days',\n",
        "                                                 'jobs_source_url_last_30_days',\n",
        "                                                 'jobs_source_url_last_7_days'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOhZjdst6jx0"
      },
      "source": [
        "## Efecto de cada variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE-dsBe_6kZo"
      },
      "outputs": [],
      "source": [
        "# A nivel académico: usaré Random forest - Da el mejor resultado preliminar\n",
        "\n",
        "target='zl_tech'\n",
        "incluir_a = ['days_since_first_date_found', 'days_since_last_date_found_source_job_url',\n",
        "             'days_since_last_date_found_source_job_title', 'days_since_last_date_found_source_job_description',\n",
        "             'days_since_last_date_any','days_since_first_date_found_source_job_description',\n",
        "             'days_since_first_date_found_source_job_title','days_since_first_date_found_source_job_url', 'keyword_id',\n",
        "             'confidence', 'is_recruiting_agency', 'jobs','jobs_last_180_days', 'jobs_last_30_days',\n",
        "             'jobs_last_7_days','jobs_source_description', 'jobs_source_description_last_180_days',\n",
        "             'jobs_source_description_last_30_days','jobs_source_description_last_7_days', 'jobs_source_title',\n",
        "             'jobs_source_title_last_180_days', 'jobs_source_title_last_30_days', 'jobs_source_title_last_7_days',\n",
        "             'jobs_source_url', 'jobs_source_url_last_180_days', 'jobs_source_url_last_30_days',\n",
        "             'jobs_source_url_last_7_days', 'technology_rank_source_jobs','technology_rank_180_days_source_jobs',\n",
        "             'rank_last_date_found_source_job_url', 'rank_1_tie_source_jobs', 'rank_180_days_tie_source_jobs',\n",
        "             'relative_occurrence_within_category_source_jobs',\n",
        "             'relative_occurrence_within_category_180_days_source_jobs']\n",
        "\n",
        "X_a = df[incluir_a]\n",
        "y_a = df[target].astype(int)\n",
        "\n",
        "# One-hot encoding\n",
        "X_encoded_a = pd.get_dummies(X_a, drop_first=True)\n",
        "\n",
        "# Train/Test\n",
        "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(\n",
        "    X_encoded_a, y_a, test_size=0.2, random_state=42, stratify=y_a)\n",
        "\n",
        "# Modelo RandomForest\n",
        "rf_a = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    random_state=42)\n",
        "rf_a.fit(X_train_a, y_train_a)\n",
        "\n",
        "# Predicciones con probabilidad\n",
        "y_pred_proba_a = rf_a.predict_proba(X_test_a)[:,1]\n",
        "\n",
        "# Ajuste de threshold\n",
        "threshold = 0.3\n",
        "y_pred_adj_a = (y_pred_proba_a >= threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "print(\"\\nMétricas con threshold ajustado:\")\n",
        "print(f\"Accuracy : {accuracy_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"F1-score : {f1_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "\n",
        "# Curva ROC para analizar tradeoff\n",
        "fpr, tpr, thresholds = roc_curve(y_test_a, y_pred_proba_a)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=\"RandomForest\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Curva ROC - RandomForest\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Umbral óptimo según F1\n",
        "f1_scores = []\n",
        "for t in thresholds:\n",
        "    y_temp_a = (y_pred_proba_a >= t).astype(int)\n",
        "    f1_scores.append(f1_score(y_test_a, y_temp_a))\n",
        "opt_threshold_a = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"\\n Threshold óptimo (según F1): {opt_threshold_a:.3f}\")\n",
        "\n",
        "# Importancia de variables\n",
        "importances_a = rf_a.feature_importances_\n",
        "features_a = X_encoded_a.columns\n",
        "\n",
        "feat_importances_a = pd.DataFrame({\n",
        "    \"Variable\": features_a,\n",
        "    \"Importancia\": importances_a}).sort_values(by=\"Importancia\", ascending=False)\n",
        "\n",
        "print(\"\\n Variables más influyentes en el modelo:\")\n",
        "print(feat_importances_a.head(30))\n",
        "\n",
        "# Gráfico\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feat_importances_a[\"Variable\"].head(30), feat_importances_a[\"Importancia\"].head(30))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Importancia\")\n",
        "plt.title(\"Top 15 Variables más influyentes - RandomForest\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4H8Au897fwY"
      },
      "outputs": [],
      "source": [
        "# A nivel práctico: usaré Random Forest\n",
        "\n",
        "incluir_p = ['is_recruiting_agency',\n",
        "             'jobs_last_180_days', 'jobs_last_30_days', 'jobs_last_7_days',\n",
        "             'jobs_source_description_last_180_days',\n",
        "             'jobs_source_description_last_30_days',\n",
        "             'jobs_source_description_last_7_days',\n",
        "             'jobs_source_title_last_180_days',\n",
        "             'jobs_source_title_last_30_days', 'jobs_source_title_last_7_days',\n",
        "             'jobs_source_url_last_180_days',\n",
        "             'jobs_source_url_last_30_days',\n",
        "             'jobs_source_url_last_7_days']\n",
        "\n",
        "X_p = df[incluir_p]\n",
        "y_p = df[target].astype(int)\n",
        "\n",
        "# One-hot encoding\n",
        "X_encoded_p = pd.get_dummies(X_p, drop_first=True)\n",
        "\n",
        "# Train/Test\n",
        "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
        "    X_encoded_p, y_p, test_size=0.2, random_state=42, stratify=y_p)\n",
        "\n",
        "# Modelo RandomForest\n",
        "rf_p = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    random_state=42)\n",
        "rf_p.fit(X_train_p, y_train_p)\n",
        "\n",
        "# Predicciones con probabilidad\n",
        "y_pred_proba_p = rf_p.predict_proba(X_test_p)[:,1]\n",
        "\n",
        "# Ajuste de threshold\n",
        "threshold = 0.3\n",
        "y_pred_adj_p = (y_pred_proba_p >= threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "print(\"\\nMétricas con threshold ajustado:\")\n",
        "print(f\"Accuracy : {accuracy_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"F1-score : {f1_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "\n",
        "# Curva ROC para analizar tradeoff\n",
        "fpr, tpr, thresholds = roc_curve(y_test_p, y_pred_proba_p)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=\"RandomForest\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Curva ROC - RandomForest\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Umbral óptimo según F1\n",
        "f1_scores = []\n",
        "for t in thresholds:\n",
        "    y_temp_p = (y_pred_proba_p >= t).astype(int)\n",
        "    f1_scores.append(f1_score(y_test_p, y_temp_p))\n",
        "opt_threshold_p = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"\\n Threshold óptimo (según F1): {opt_threshold_p:.3f}\")\n",
        "\n",
        "# Importancia de variables\n",
        "importances_p = rf_p.feature_importances_\n",
        "features_p = X_encoded_p.columns\n",
        "\n",
        "feat_importances_p = pd.DataFrame({\n",
        "    \"Variable\": features_p,\n",
        "    \"Importancia\": importances_p}).sort_values(by=\"Importancia\", ascending=False)\n",
        "\n",
        "print(\"\\n Variables más influyentes en el modelo:\")\n",
        "print(feat_importances_p.head(30))\n",
        "\n",
        "# Gráfico\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feat_importances_p[\"Variable\"].head(30), feat_importances_p[\"Importancia\"].head(30))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Importancia\")\n",
        "plt.title(\"Top 15 Variables más influyentes - RandomForest\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbQtmYiv7jsv"
      },
      "source": [
        "## Optimizar modelo seleccionando variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i-UpU0u7mYl"
      },
      "outputs": [],
      "source": [
        "#Modelo Académico\n",
        "incluir_a = ['days_since_first_date_found',\n",
        "             'days_since_first_date_found_source_job_description',\n",
        "             'keyword_id',\n",
        "             'days_since_last_date_any',\n",
        "             'days_since_last_date_found_source_job_description',\n",
        "             'jobs_source_description',\n",
        "             'relative_occurrence_within_category_source_jobs',\n",
        "             'jobs',\n",
        "             'jobs_last_180_days']\n",
        "\n",
        "X_a = df[incluir_a]\n",
        "y_a = df[target].astype(int)\n",
        "\n",
        "# One-hot encoding\n",
        "X_encoded_a = pd.get_dummies(X_a, drop_first=True)\n",
        "\n",
        "# Train/Test\n",
        "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(\n",
        "    X_encoded_a, y_a, test_size=0.2, random_state=42, stratify=y_a)\n",
        "\n",
        "# Modelo RandomForest\n",
        "rf_a = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    random_state=42)\n",
        "rf_a.fit(X_train_a, y_train_a)\n",
        "\n",
        "# Predicciones con probabilidad\n",
        "y_pred_proba_a = rf_a.predict_proba(X_test_a)[:,1]\n",
        "\n",
        "# Ajuste de threshold\n",
        "threshold = 0.3\n",
        "y_pred_adj_a = (y_pred_proba_a >= threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "print(\"\\nMétricas con threshold ajustado:\")\n",
        "print(f\"Accuracy : {accuracy_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "print(f\"F1-score : {f1_score(y_test_a, y_pred_adj_a):.4f}\")\n",
        "\n",
        "# Curva ROC para analizar tradeoff\n",
        "fpr, tpr, thresholds = roc_curve(y_test_a, y_pred_proba_a)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=\"RandomForest\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Curva ROC - RandomForest\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Umbral óptimo según F1\n",
        "f1_scores = []\n",
        "for t in thresholds:\n",
        "    y_temp_a = (y_pred_proba_a >= t).astype(int)\n",
        "    f1_scores.append(f1_score(y_test_a, y_temp_a))\n",
        "opt_threshold_a = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"\\n Threshold óptimo (según F1): {opt_threshold_a:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQmoYgXd7rmO"
      },
      "outputs": [],
      "source": [
        "#Modelo Práctico optimizado\n",
        "incluir_p = ['jobs_source_description_last_180_days',\n",
        "             'jobs_last_180_days',\n",
        "             'jobs_last_30_days',\n",
        "             'jobs_source_description_last_30_days',\n",
        "             'jobs_source_description_last_7_days',\n",
        "             'jobs_last_7_days',\n",
        "             'jobs_source_title_last_180_days']\n",
        "\n",
        "X_p = df[incluir_p]\n",
        "y_p = df[target].astype(int)\n",
        "\n",
        "# One-hot encoding\n",
        "X_encoded_p = pd.get_dummies(X_p, drop_first=True)\n",
        "\n",
        "# Train/Test\n",
        "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
        "    X_encoded_p, y_p, test_size=0.2, random_state=42, stratify=y_p)\n",
        "\n",
        "#Preprocesamiento\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', StandardScaler(), X_p.select_dtypes(include=np.number).columns),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), X_p.select_dtypes(include='object').columns)])\n",
        "\n",
        "# Crear el pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', rf_p)])\n",
        "\n",
        "# Entrenar el pipeline con todos los datos\n",
        "pipeline.fit(X_p, y_p)\n",
        "\n",
        "# Modelo RandomForest\n",
        "rf_p = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    random_state=42)\n",
        "rf_p.fit(X_train_p, y_train_p)\n",
        "\n",
        "# Predicciones con probabilidad\n",
        "y_pred_proba_p = rf_p.predict_proba(X_test_p)[:,1]\n",
        "\n",
        "# Ajuste de threshold\n",
        "threshold = 0.3\n",
        "y_pred_adj_p = (y_pred_proba_p >= threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "print(\"\\nMétricas con threshold ajustado:\")\n",
        "print(f\"Accuracy : {accuracy_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "print(f\"F1-score : {f1_score(y_test_p, y_pred_adj_p):.4f}\")\n",
        "\n",
        "# Curva ROC para analizar tradeoff\n",
        "fpr, tpr, thresholds = roc_curve(y_test_p, y_pred_proba_p)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=\"RandomForest\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Curva ROC - RandomForest\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Umbral óptimo según F1\n",
        "f1_scores = []\n",
        "for t in thresholds:\n",
        "    y_temp_p = (y_pred_proba_p >= t).astype(int)\n",
        "    f1_scores.append(f1_score(y_test_p, y_temp_p))\n",
        "opt_threshold_p = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"\\n Threshold óptimo (según F1): {opt_threshold_p:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5C6Q6ZX72r0"
      },
      "source": [
        "# Productivización del modelo (práctico)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ETju3x-77G6"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken 325KD5cOKlBGzVkYLxGGy3gjqIn_7KCV6t5vbmU7dKmC5oH56\n",
        "app = Flask(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU0_3FoD77vR"
      },
      "outputs": [],
      "source": [
        "# Guardar el modelo entrenado\n",
        "joblib.dump(pipeline, 'modelo_practico_optimizado.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB9JeWp37-Fi"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo entrenado\n",
        "\n",
        "try:\n",
        "    pipeline = joblib.load('modelo_practico_optimizado.pkl')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: modelo_practico_optimizado.pkl not found.\")\n",
        "    pipeline = None\n",
        "\n",
        "# Ruta\n",
        "if pipeline is not None:\n",
        "    @app.route('/predigo', methods=['POST'])\n",
        "    def predigo():\n",
        "        data = request.get_json(force=True)\n",
        "        df_predict = pd.DataFrame([data])\n",
        "        preds = pipeline.predict(df_predict)\n",
        "        return jsonify({'prediction': int(preds[0])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8unUyam8EBO"
      },
      "outputs": [],
      "source": [
        "# Abrir un túnel público\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"La API está disponible en:\", public_url)\n",
        "\n",
        "# Levantar Flask\n",
        "app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VDbKJ0f74fhN",
        "323kk7yZ4kFn",
        "VKcZK0-s49h-",
        "E0qqv5nF5X6j",
        "6EoPI6mJ5nte",
        "eAnXDu0a6LfF",
        "8xE9iAiJ6Ot0",
        "WOhZjdst6jx0",
        "hbQtmYiv7jsv",
        "j5C6Q6ZX72r0"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}